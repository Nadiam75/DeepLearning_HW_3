{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu9EnqB32pVD",
        "outputId": "a8312a46-4c37-434a-cf8e-fc9fabf0674b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX0uW6j74a-r",
        "outputId": "ca3364fa-444b-46ef-9e6e-a3cba5f33fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QHhi5MzS2Os9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline\n",
        "sns.set(color_codes=True)\n",
        "from transformers import BertTokenizer,BertForSequenceClassification,AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDSB219qHMd-",
        "outputId": "e6edecb6-b08d-4642-e066-3124e2765b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/trainingandtestdata.zip\n",
            "replace testdata.manual.2009.06.14.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!unzip drive/MyDrive/trainingandtestdata.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9OfEW2VmHYet"
      },
      "outputs": [],
      "source": [
        "testData = pd.read_csv('testdata.manual.2009.06.14.csv')\n",
        "trainData = pd.read_csv('training.1600000.processed.noemoticon.csv' ,  encoding='latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GV9SNLP4HYev"
      },
      "outputs": [],
      "source": [
        "columnNames = ['polarity' , 'id' , 'date' , 'query' , 'user', 'text']\n",
        "trainData.columns = columnNames\n",
        "testData.columns = columnNames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "14F7l9raHYew"
      },
      "outputs": [],
      "source": [
        "testData = testData.loc[testData[\"polarity\"] != 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C9r-5W0uHYew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb885614-1e49-4a7e-906e-b277fb70cd51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value)\n"
          ]
        }
      ],
      "source": [
        "testData.loc[testData['polarity'] == 4, 'polarity'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YghszLGbHYex"
      },
      "outputs": [],
      "source": [
        "trainData.loc[trainData['polarity'] == 4, 'polarity'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zpG9HXRRHYex"
      },
      "outputs": [],
      "source": [
        "# columnNames = ['polarity' , 'id' , 'date' , 'query' , 'user', 'text']\n",
        "del testData['id']\n",
        "del testData['date']\n",
        "del testData['query']\n",
        "del testData['user']\n",
        "del trainData ['id']\n",
        "del trainData['date']\n",
        "del trainData['query']\n",
        "del trainData['user']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD52dtjNH79v",
        "outputId": "77368725-b173-475f-b93e-e887781e0b13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((358, 2), (1599999, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "testData.shape  , trainData.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3EtiGJOe2jdS",
        "outputId": "7463e9fa-92e9-441f-d870-c39875c44588"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e1836f1e-2325-4e66-b264-18895174144e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kwesidei not the whole crew</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1836f1e-2325-4e66-b264-18895174144e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1836f1e-2325-4e66-b264-18895174144e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1836f1e-2325-4e66-b264-18895174144e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   polarity                                               text\n",
              "0         0  is upset that he can't update his Facebook by ...\n",
              "1         0  @Kenichan I dived many times for the ball. Man...\n",
              "2         0    my whole body feels itchy and like its on fire \n",
              "3         0  @nationwideclass no, it's not behaving at all....\n",
              "4         0                      @Kwesidei not the whole crew "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "trainData.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " trainData = trainData.reindex(np.random.permutation(trainData.index))"
      ],
      "metadata": {
        "id": "bzsQo-eaRGcK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainData.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HpVNl2etRL9p",
        "outputId": "44af8b52-1232-4803-8a55-aa68a6ad70cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a92d6e77-c51a-4aee-a4c1-fc5d10bc6f72\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>426147</th>\n",
              "      <td>0</td>\n",
              "      <td>@sweetoblivion26 That sounds pretty awful  I'm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638984</th>\n",
              "      <td>0</td>\n",
              "      <td>@OfficerAnni you're so confused that the unthi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984520</th>\n",
              "      <td>1</td>\n",
              "      <td>@Noway57 Haha...I'm a fan since I was in prima...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1326164</th>\n",
              "      <td>1</td>\n",
              "      <td>http://tinyurl.com/m3pksa Microsoft 'Natal' - ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1549611</th>\n",
              "      <td>1</td>\n",
              "      <td>@gizelleramos oh it sounded like you had summe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a92d6e77-c51a-4aee-a4c1-fc5d10bc6f72')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a92d6e77-c51a-4aee-a4c1-fc5d10bc6f72 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a92d6e77-c51a-4aee-a4c1-fc5d10bc6f72');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         polarity                                               text\n",
              "426147          0  @sweetoblivion26 That sounds pretty awful  I'm...\n",
              "638984          0  @OfficerAnni you're so confused that the unthi...\n",
              "984520          1  @Noway57 Haha...I'm a fan since I was in prima...\n",
              "1326164         1  http://tinyurl.com/m3pksa Microsoft 'Natal' - ...\n",
              "1549611         1  @gizelleramos oh it sounded like you had summe..."
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f2Jh8i_u2kqf",
        "outputId": "807bf2e5-8328-4a9b-8eef-8de22fa86be6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-44255bf1-bc78-4c40-ab2c-c255dab50814\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44255bf1-bc78-4c40-ab2c-c255dab50814')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-44255bf1-bc78-4c40-ab2c-c255dab50814 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-44255bf1-bc78-4c40-ab2c-c255dab50814');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   polarity                                               text\n",
              "0         1  Reading my kindle2...  Love it... Lee childs i...\n",
              "1         1  Ok, first assesment of the #kindle2 ...it fuck...\n",
              "2         1  @kenburbary You'll love your Kindle2. I've had...\n",
              "3         1  @mikefish  Fair enough. But i have the Kindle2...\n",
              "4         1  @richardebaker no. it is too big. I'm quite ha..."
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "testData.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " testData = testData.reindex(np.random.permutation(testData.index))"
      ],
      "metadata": {
        "id": "wXKOQyfTJ3s-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testData.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2MZ8zFjDJ6nf",
        "outputId": "b0f97a04-f7c1-4cac-8b1b-508f81efe169"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bb6b8fe3-1787-4795-9b5b-3bbe36ace774\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>1</td>\n",
              "      <td>I love my Kindle2. No more stacks of books to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>0</td>\n",
              "      <td>Having the old Coca-Cola guy on the GM board i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0</td>\n",
              "      <td>It's a bank holiday, yet I'm only out of work ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>1</td>\n",
              "      <td>Will the Lakers kick the Nuggets ass tonight?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>0</td>\n",
              "      <td>Just blocked United Blood Services using Googl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb6b8fe3-1787-4795-9b5b-3bbe36ace774')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bb6b8fe3-1787-4795-9b5b-3bbe36ace774 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bb6b8fe3-1787-4795-9b5b-3bbe36ace774');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     polarity                                               text\n",
              "455         1  I love my Kindle2. No more stacks of books to ...\n",
              "377         0  Having the old Coca-Cola guy on the GM board i...\n",
              "98          0  It's a bank holiday, yet I'm only out of work ...\n",
              "326         1      Will the Lakers kick the Nuggets ass tonight?\n",
              "221         0  Just blocked United Blood Services using Googl..."
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "feT_RESd3tGi"
      },
      "outputs": [],
      "source": [
        "# Defining dictionary containing all emojis with their meanings.\n",
        "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
        "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
        "\n",
        "## Defining set containing all stopwords in english.\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b2d840-f00b-4d75-a49c-5e8fa1e2ee63",
        "id": "bFW_qKtft_Mv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess(textdata):\n",
        "    processedText = []\n",
        "    \n",
        "    # Create Lemmatizer and Stemmer.\n",
        "    # wordLemm = WordNetLemmatizer()\n",
        "    \n",
        "    # Defining regex patterns.\n",
        "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    userPattern       = '@[^\\s]+'\n",
        "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "    sequencePattern   = r\"(.)\\1\\1+\"\n",
        "    seqReplacePattern = r\"\\1\\1\"\n",
        "    \n",
        "    # for tweet in textdata:\n",
        "    tweet =textdata\n",
        "    # print(tweet)\n",
        "    tweet = tweet.lower()\n",
        "    \n",
        "    # Replace all URls with 'URL'\n",
        "    tweet = re.sub(urlPattern,' URL',tweet)\n",
        "    # Replace all emojis.\n",
        "    for emoji in emojis.keys():\n",
        "        tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
        "    # Replace @USERNAME to 'USER'.\n",
        "    tweet = re.sub(userPattern,' USER', tweet)        \n",
        "    # Replace all non alphabets.\n",
        "    tweet = re.sub(alphaPattern, \" \", tweet)\n",
        "    # Replace 3 or more consecutive letters by 2 letter.\n",
        "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "\n",
        "    tweetwords = ''\n",
        "    for word in tweet.split():\n",
        "        # Checking if the word is a stopword.\n",
        "        #if word not in stopwordlist:\n",
        "        if len(word)>1:\n",
        "            # Lemmatizing the word.\n",
        "            # word = wordLemm.lemmatize(word)\n",
        "            tweetwords += (word+' ')\n",
        "        \n",
        "    processedText.append(tweetwords)\n",
        "        \n",
        "    return processedText[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NeKq3pWK2_PA"
      },
      "outputs": [],
      "source": [
        "trainData['text'] = trainData.text.apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "akaI7rc23myZ"
      },
      "outputs": [],
      "source": [
        "testData['text'] = testData.text.apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gWAnCD7s3s-5",
        "outputId": "29e6b011-b29e-4244-fe4d-0b39edc4c99a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-49928905-9722-4487-99e1-fd3c2a041b5d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>426147</th>\n",
              "      <td>0</td>\n",
              "      <td>USER that sounds pretty awful so sorry haven g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638984</th>\n",
              "      <td>0</td>\n",
              "      <td>USER you re so confused that the unthinkable h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984520</th>\n",
              "      <td>1</td>\n",
              "      <td>USER haha fan since was in primary school haha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1326164</th>\n",
              "      <td>1</td>\n",
              "      <td>URL microsoft natal will this actually work wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1549611</th>\n",
              "      <td>1</td>\n",
              "      <td>USER oh it sounded like you had summer school ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49928905-9722-4487-99e1-fd3c2a041b5d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49928905-9722-4487-99e1-fd3c2a041b5d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49928905-9722-4487-99e1-fd3c2a041b5d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         polarity                                               text\n",
              "426147          0  USER that sounds pretty awful so sorry haven g...\n",
              "638984          0  USER you re so confused that the unthinkable h...\n",
              "984520          1  USER haha fan since was in primary school haha...\n",
              "1326164         1  URL microsoft natal will this actually work wi...\n",
              "1549611         1  USER oh it sounded like you had summer school ..."
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "trainData.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testData.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-cmr1jfZyUhq",
        "outputId": "ad058cf3-d519-42d4-b5c0-67aa8fba24a8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e1a30295-3b63-4389-9a8b-bed910915b14\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>1</td>\n",
              "      <td>love my kindle2 no more stacks of books to tri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>0</td>\n",
              "      <td>having the old coca cola guy on the gm board i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0</td>\n",
              "      <td>it bank holiday yet only out of work now exam ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>1</td>\n",
              "      <td>will the lakers kick the nuggets ass tonight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>0</td>\n",
              "      <td>just blocked united blood services using googl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1a30295-3b63-4389-9a8b-bed910915b14')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1a30295-3b63-4389-9a8b-bed910915b14 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1a30295-3b63-4389-9a8b-bed910915b14');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     polarity                                               text\n",
              "455         1  love my kindle2 no more stacks of books to tri...\n",
              "377         0  having the old coca cola guy on the gm board i...\n",
              "98          0  it bank holiday yet only out of work now exam ...\n",
              "326         1      will the lakers kick the nuggets ass tonight \n",
              "221         0  just blocked united blood services using googl..."
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XH-Djkz43svr"
      },
      "outputs": [],
      "source": [
        "train_labels = trainData.polarity.values\n",
        "train_text = trainData.text.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6jVXNzwm3sn3"
      },
      "outputs": [],
      "source": [
        "test_labels = testData.polarity.values\n",
        "test_text = testData.text.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xusxux944ElU"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw_Zl-jU4Eq0",
        "outputId": "ec99280b-4d73-4ae4-f313-4e3a83f806be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "input_ids_ = []\n",
        "attention_mask_ = []\n",
        "for i in train_text:\n",
        "    encoded_data = tokenizer.encode_plus( i, add_special_tokens=True, max_length=64, pad_to_max_length = True, return_attention_mask= True,\n",
        "    return_tensors='pt')\n",
        "    input_ids_.append(encoded_data['input_ids'])\n",
        "    attention_mask_.append(encoded_data['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.cat(input_ids_,dim=0)\n",
        "attention_mask = torch.cat(attention_mask_,dim=0)\n",
        "train_labels = torch.tensor(train_labels)"
      ],
      "metadata": {
        "id": "jdu8gdXKn7yu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uKozBDsp4EwA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader,SequentialSampler,RandomSampler,TensorDataset,random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "X7qHkhTx5nEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4896a2a1-8a7b-40ae-ee28-2d45eff00809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Size -  80000\n",
            "Validation Size -  20000\n"
          ]
        }
      ],
      "source": [
        "num = 100000\n",
        "dataset = TensorDataset(input_ids[:num],attention_mask[:num],train_labels[:num])\n",
        "train_size = int(0.8*len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset,val_dataset = random_split(dataset,[train_size,val_size])\n",
        "\n",
        "print('Training Size - ',train_size)\n",
        "print('Validation Size - ',val_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "c4AppIAy5nZP"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_dataset,sampler = RandomSampler(train_dataset),\n",
        "                     batch_size = 64)\n",
        "val_dl = DataLoader(val_dataset,sampler = SequentialSampler(val_dataset),\n",
        "                     batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AxL_584q5ng5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22108a3-4532-4600-edeb-b530614bbc19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1250, 313)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "len(train_dl),len(val_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-jnBAS9f5nnl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84758de6-b932-4c68-d652-6fb4bd6504fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained( 'bert-base-uncased', num_labels = 2, output_attentions = False, output_hidden_states = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "7PqMI2xn2_Tx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VoG4Pl-p2_ZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "197315fb-b4d5-4192-fcce-1ecd09bd8ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "o8P9DSEH2_hE"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(),lr = 2e-5,eps=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 4\n",
        "total_steps = len(train_dl)*epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,\n",
        "                                           num_training_steps=total_steps)"
      ],
      "metadata": {
        "id": "vTd5HYyahTCI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds,labels):\n",
        "    pred_flat = np.argmax(preds,axis=1).flatten()\n",
        "    label_flat = labels.flatten()\n",
        "    return np.sum(pred_flat==label_flat)/len(label_flat)"
      ],
      "metadata": {
        "id": "prfjQHgohTHW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader_test):\n",
        "    model.eval()\n",
        "    loss_val_total = 0\n",
        "    predictions,true_vals = [],[]\n",
        "    for batch in dataloader_test:\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        inputs = {\n",
        "            'input_ids':batch[0],\n",
        "            'attention_mask': batch[1],\n",
        "            'labels': batch[2]\n",
        "        }\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "    loss_val_avg = loss_val_total / len(dataloader_test)\n",
        "    predictions = np.concatenate(predictions,axis=0)\n",
        "    true_vals = np.concatenate(true_vals,axis=0)\n",
        "    return loss_val_avg,predictions,true_vals"
      ],
      "metadata": {
        "id": "UYbLVauUhTNP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "validation_losses = []\n",
        "accuracies = []\n",
        "from tqdm.notebook import tqdm\n",
        "torch.cuda.empty_cache()\n",
        "for epoch in tqdm(range(1, epochs+1)):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    loss_train_total = 0\n",
        "\n",
        "    progress_bar = tqdm(train_dl, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
        "    for batch in progress_bar:\n",
        "\n",
        "        model.zero_grad()\n",
        "        \n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }       \n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "\n",
        "\n",
        "        loss_train_total += loss.item()\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "\n",
        "        \n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        # train_loss, predictions_train, true_train = evaluate(train_dl)\n",
        "        # train_accuracy = accuracy (predictions_train  , true_train)\n",
        "\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
        "         \n",
        "        \n",
        "    tqdm.write(f'\\nEpoch {epoch}')\n",
        "    \n",
        "    loss_train_avg = loss_train_total/len(train_dl)            \n",
        "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
        "    \n",
        "    val_loss, predictions, true_vals = evaluate(val_dl)\n",
        "    validation_losses.append(val_loss)\n",
        "    val_acc = accuracy(predictions, true_vals)\n",
        "    tqdm.write(f'Validation loss: {val_loss}')\n",
        "    tqdm.write(f'Accuracy: {val_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "c244499731494fef8754ceae12c1793e",
            "a7cd909c1e2149e5a0e23957655b6522",
            "97840aad44bf45db8af0d90c630b1f39",
            "dca5ef8d2bed478884cf32d1a68ef8e7",
            "0e80889898f4485a996fe80c44fc022c",
            "3fb51e5626054ca0bb0fd5fdbc889b05",
            "13be6818b28c4750bf63bb2fd404019a",
            "d7985c41f0454aae85405358447d9cef",
            "4ba23c1f353540499b92b8621a105731",
            "c6433b44194144d0bb947c0b27289fb3",
            "ea4cb909591e4219a4f7f02fc7f48740",
            "8283b7b96fef4cba9262ce0f5576bf4e",
            "50efdedc977a42808251f517c51f110d",
            "5a2cfbdd82fd4093a8cee3e7f1e8fbf4",
            "138e2aad057841dcb3323e1919a5062f",
            "5238a60e80c241b2b1b011ddff8e71d9",
            "a85bb75932bb4eb7b2ae51a8c7197738",
            "d853dbb8191743ac903f92a4b1c8beec",
            "37b0554289d245e791726554f0300bea",
            "689f0bf5654d40678457e0df725e19e4",
            "2504636c4a78435c98c92461788f2902",
            "fa6b5abaa22842c2b89268ec5930ce43",
            "094441f51c264c9a8cff68d79f32f56f",
            "7f9f5f78f6904377af4926e50c2444c0",
            "502612f319ab4e988547188e4e2f96e3",
            "6324f50557b74eff9a2d2c163f458a20",
            "19846a40951e48d7b5f66a40f77afad6",
            "43122270965d44079223700bc3e9f583",
            "47c0b6cf212748908e3db291f558a217",
            "23023de4995c477bbd6ef9c5b667855d",
            "2afc5e72923940bda9e19307d51a5fd3",
            "dabafaf292414b9f8b11ac3923ba56ad",
            "21e1868c2a414326be41adbd4b2c0435",
            "21fc96744440451998899eff00e9c2c9",
            "c177badb09f14062822a882357f63303",
            "7411c21bacd74564b323fa7c14bc18bf",
            "8b139b9bf7944caaaf25f36382640c9d",
            "8c7a76d10e774ce5aa2f9329114f371d",
            "385ee47cd082466ba49fe23e48b57c93",
            "0c7e8f6985c0488599549eec012e5187",
            "1cb58f3f35564d50b5b4031417f29c85",
            "8e33fe405e76451e8caa5bb898f30616",
            "f4d7b70d27474a2a8c561714ab7f9d3b",
            "44e963627f8844d2b564dbf2fd157b8e",
            "86bb74860ec04cd085d1b9db531e37bc",
            "7db8db6d4e534c4f92990525c187568c",
            "0c7733533cda48f68a3abbd463bd9b99",
            "e8368a1c90034c1c9e90bc53bc5925a8",
            "2bb60c6680b642c58ca338e76dee8c5c",
            "c89590ee875f4e75bf3e53deb63a544b",
            "60afa2ec1a344dca92ec7d949cb599f4",
            "013f0a8214f04644b3d000eeea92dd04",
            "4cc4bf368c66426eb45817f0d709ae13",
            "7a563b6c2d204364b4a551ec4980c186",
            "3992c196225642b78833c52f10fe0632"
          ]
        },
        "id": "oLKgMQZNhTTY",
        "outputId": "5f047d0e-ebc3-4168-bc18-d1f4d0f81077"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c244499731494fef8754ceae12c1793e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8283b7b96fef4cba9262ce0f5576bf4e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epoch 1:   0%|          | 0/1250 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Training loss: 0.3222084024488926\n",
            "Validation loss: 0.37262657560860385\n",
            "Accuracy: 0.841\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "094441f51c264c9a8cff68d79f32f56f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epoch 2:   0%|          | 0/1250 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2\n",
            "Training loss: 0.24739518510103226\n",
            "Validation loss: 0.39520904750298386\n",
            "Accuracy: 0.8402\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21fc96744440451998899eff00e9c2c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epoch 3:   0%|          | 0/1250 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3\n",
            "Training loss: 0.19287507625222206\n",
            "Validation loss: 0.4553220868110657\n",
            "Accuracy: 0.83785\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86bb74860ec04cd085d1b9db531e37bc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epoch 4:   0%|          | 0/1250 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4\n",
            "Training loss: 0.1738841490983963\n",
            "Validation loss: 0.4553220868110657\n",
            "Accuracy: 0.83785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "accs = [0.841 , 0.8402 ,  0.83785 , 0.83785]\n",
        "plt.plot(accs)\n",
        "plt.ylabel('metrics')\n",
        "plt.xlabel('iteration')\n",
        "plt.legend(['accuracy'])\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nAgJZxmxs5uY",
        "outputId": "8384bea6-14ee-4392-e027-675b99c86f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(validation_losses)\n",
        "plt.ylabel('metrics')\n",
        "plt.xlabel('iteration')\n",
        "plt.legend(['validation loss'])\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kWRTG57TRv13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.ylabel('metrics')\n",
        "plt.xlabel('iteration')\n",
        "plt.legend(['loss'])\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kfJ1tfCVkQdu",
        "outputId": "a0abf119-7f03-4922-8e4b-ec7506ad56d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = './'\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgqc7ufDhTY4",
        "outputId": "da7093fd-3595-4c78-f8b2-b0535f15eed0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./tokenizer_config.json',\n",
              " './special_tokens_map.json',\n",
              " './vocab.txt',\n",
              " './added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer,BertForSequenceClassification\n",
        "import torch\n",
        "print('Loading BERT tokenizer...')\n",
        "output_dir = './'\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "model_loaded = BertForSequenceClassification.from_pretrained(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlVqNuaphb3l",
        "outputId": "35f55603-0c6d-4759-8ca2-98816ddd0024"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predictSentiment(sent):\n",
        "    output_dir = './'\n",
        "    tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "    model_loaded = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "    encoded_dict = tokenizer.encode_plus( sent, add_special_tokens = True, max_length = 64, pad_to_max_length = True,\n",
        "                        return_attention_mask = True, return_tensors = 'pt', )\n",
        "        \n",
        "    input_id = encoded_dict['input_ids']\n",
        "\n",
        "    attention_mask = encoded_dict['attention_mask']\n",
        "    input_id = torch.LongTensor(input_id)\n",
        "    attention_mask = torch.LongTensor(attention_mask)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_loaded = model_loaded.to(device)\n",
        "    input_id = input_id.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model_loaded(input_id, token_type_ids=None, attention_mask=attention_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "    index = logits.argmax()\n",
        "    return index"
      ],
      "metadata": {
        "id": "rUj69gtxhb8O"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = predictSentiment('she seems really happy')\n",
        "if ans == 1:\n",
        "    print(\"Positive\")\n",
        "else:\n",
        "    print(\"Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb_0O2iPhcHg",
        "outputId": "dec13fa5-efb7-4b60-fdcc-548a1b7a7087"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fQEgu1tliQwO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = predictSentiment('he hates everyone')\n",
        "if ans == 1:\n",
        "    print(\"Positive\")\n",
        "else:\n",
        "    print(\"Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgNpgwkjLYrb",
        "outputId": "ea0330c8-f2d0-4a36-c0e4-cb8d8cbfae58"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = predictSentiment('it was a wonderful event')\n",
        "if pred == 1:\n",
        "    print(\"Positive\")\n",
        "else:\n",
        "    print(\"Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzGEFZ_SOlbj",
        "outputId": "0fe74cf5-28cb-4365-e2a8-acde132ba0ff"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for row in testData.iterrows():\n",
        "  selected_sentence = row[1]['text']\n",
        "  pred = predictSentiment(selected_sentence)\n",
        "  predictions.append(pred.item())\n",
        "\n",
        "  if pred == 1 :\n",
        "    result = 'positive'\n",
        "  else:\n",
        "    result = 'negative'\n",
        "  if  row[1]['polarity'] == 1:\n",
        "    polarity = 'positive'\n",
        "  else: \n",
        "    polarity = 'negative'\n",
        "  \n",
        "  print(\"selected sentence: \" , selected_sentence)\n",
        "  print(\"actual label=  \" , polarity)\n",
        "  print(\"predicted label= \" , result)\n",
        "  print(\"********************************************************************\")"
      ],
      "metadata": {
        "id": "E7L54ojYLa3L",
        "outputId": "b5a2e5f0-f1c6-4e26-cdd6-760e6941de65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  love my kindle2 no more stacks of books to trip over on the way to the loo \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  having the old coca cola guy on the gm board is stupid has heck tcot ala \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  it bank holiday yet only out of work now exam season sucksEMOJIsad \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  will the lakers kick the nuggets ass tonight \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just blocked united blood services using google voice they call more than those car warranty guys \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  back when worked for nike we had one fav word just do it EMOJIsmile \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  itchy back dont ya hate it \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  hello twitter api EMOJIwink \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  good news just had call from the visa office saying everything is fine what relief am sick of scams out there stealing \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  wth have never seen line this loong at time warner before ugh \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  can wait for the great american food and music festival at shoreline tomorrow mm katz pastrami and bobby flay yes please \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  it is shame about gm what if they are forced to make only cars the white house thinks will sell what do you think \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  whoever is running time warner needs to be repeatedly raped by rhino so they understand the consequences of putting out shitty cable svcs \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER wtf is the point of deleting tweets if they can still be found in summize and searches twitter please fix that thanks and bye \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  omg the commercials alone on espn are going to drive me nuts \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  can wait going to see star trek tonight \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  wat the heck is north korea doing they just conducted powerful nuclear tests follow the link URL \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  according to the create school notre dame will have receivers in ncaa 10 at 84 or higher rating EMOJIsmile sweet \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  boarding plane for san francisco in hour hr flight blech \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just changed my default pic to nike basketball cause bball is awesome \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  soo dissapointed they sent danny gokey home you still rock danny my hometown hero yeah milrockee \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  is being fucked by time warner cable didnt know modems could explode and susan boyle sucks too \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  time warner has the worse customer service ever will never use them again \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rt USER awesome jquery reference book for coda URL webdesign \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  is lovin his nike already and that only from running on the spot in his bedroom \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  reading on my new kindle2 \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rt USER five things wolfram alpha does better and vastly different than google URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just realized we three monkeys in the white obama biden pelosi sarah palin 2012 \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  give man fish feed him for the day teach him to fish feed him for life buy him gm and him over for good \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  my wrist still hurts have to get it looked at hate the dr dentist scary places EMOJIsad time to watch eagle eye if you want to join txt \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  saw night at the museum last night pretty crazy movie but the cast was awesome so it was well worth it robin williams forever \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  monday already iran may implode kitchen is disaster USER seems happy USER had nice weekend and USER is great whoop \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER ooh what model are you getting have the 40d and love love love love it \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  my dentist appt today was actually quite enjoyable \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  you guys see this why does time warner have to suck so much ass really wish could get verse at my apartment URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just got barraged by horde of insects hungry for my kitchen light so scary \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  as may have noticed not too happy about the gm situation nor aig lehman et al \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  gonna go see bobby flay 2moro at shoreline eat and drink gonna be good \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER oh those are awesome so wish they weren owned by nike EMOJIsad \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  could time warner cable suck more no \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER right lol we ll get there have high expectations warren buffet style \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  obama got jokes haha just got to watch bit of his after dinner speech from last night in love with mr president EMOJIwink \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER no lebron is the best \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  still love my kindle2 but reading the new york times on it does not feel natural miss the bloomingdale ads \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  goodby silverstein agency new site URL great \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  itchy and miserable \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  we went to stanford university today got tour made me want to go back to college it also decided all of our kids will go there \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  no watching the night at the museum getting really good \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER we tried but time warner wasn being nice so we recorded today EMOJIsmile \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  fuck this economy hate aig and their non loan given asses \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  am furious with time warner and their phone promotions \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER umm having some time warner problems \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  can watch tv without tivo and after all these years the time warner dvr still sucks URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER current use the nikon d90 and love it but not as much as the canon 40d 50d chose the d90 for the video feature my mistake \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER dood got free google android phone at the conference the g2 \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  ok first assesment of the kindle2 it fucking rocks \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER hate going to the dentist \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  north korea please cease this douchebaggery china doesn even like you anymore URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER lebron is hometown hero to me lol love the lakers but let go cavs lol \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  hate time warner soo wish had vios cant watch the fricken mets game buffering feel like im watching free internet porn \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  comcast sucks \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  omgg ohhdee want mcdonalds damn wonder if its open lol \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  super investors great weekend read here from warren buffet oldie but goodie URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the real aig scandal URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just got mcdonalds goddam those eggs make me sick yeah laker up date go lakers not much of an update well it true so suck it \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  star trek was as good as everyone said \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  obama speech was pretty awesome last night URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER oh yes but if gm dies it will only be worth more boo hahaha \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  is it me or is this the best the playoffs have been in years oh yea lebron and melo in the finals \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  colin powell rocked yesterday on cbs cheney needs to shut the hell up and go home powell is man of honor and served our country proudly \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  goodby silverstein new site URL enjoy it \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  new nike muppet commercials are pretty cute why do we live together again \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  obama is quite good comedian check out his dinner speech on cnn EMOJIsmile very funny jokes \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  getting ready to test out some burger receipes this weekend bobby flay has some great receipes to try thanks bobby \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  wolfram alpha sucks even for researchers the information provided is less than you can get from google or wikipedia totally useless \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  can we just go ahead and blow north korea off the map already \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  started to think that citi is in really deep amp are they gonna survive the turmoil or are they gonna be the next aig \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just got my new toy canon 50d love love love it \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  arg twitter api is making me crazy \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  stopped to have lunch at mcdonalds chicken nuggetss EMOJIsmile yummy \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  played with an android google phone the slide out screen scares me would break that fucker so fast still prefer my iphone \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER it definitely under warranty amp my experience is the amazon support for kindle is great had to contact them about my kindle2 \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  was talking to this guy last night and he was telling me that he is die hard spurs fan he also told me that he hates lebron james \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER lebron is the boss \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  wow everyone at the google conference got free g2 with month of unlimited service \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  is upset about the whole gm thing life as know it is so screwed up \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER its not so much my obsession with cell phones but the iphone slave to at amp forever because of it EMOJIsmile \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  loving my new kindle2 named her kendra in case were wondering the cookbook is the tool cuz it tells all the tricks best gift evr \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER gm good riddance sad though \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  ooh north korea is in troublee URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  will never buy government motors vehicle until just recently drove gm cars since 1988 when bought URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  higher physics exam tommorow not lookin forward to it much EMOJIsad \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  it unfortunate that after the stimulus plan was put in place twice to help gm on the back of the american people has led to the inevitable \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  talk is cheap bing that ll stick with google URL \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  have google addiction thank you for pointing that out USER hahaha \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  nike rocks super grateful for what ve done with them EMOJIsmile amp the european division of nike is beyond USER USER \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER have the kindle2 ve seen pictures of the dx but haven seen it in person love my kindle on it everyday \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  took the graduate field exam for computer science today nothing makes you feel like more of an idiot than lambda calculus \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  class the 50d is supposed to come today EMOJIsmile \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  naive bayes using em for text classification really frustrating \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  up big or go home aig \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  at amp is complete fail \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  say some sports writers are idiots for saying roger federer is one of the best ever in tennis roger federer is the best ever in tennis \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  loved night at the museum \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  say we just cut out the small talk at amp new slogan you give us your money apologies to bob geldof \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER my dentist is great but she expensive \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  nancy pelosi gave the worst commencement speech ve ever heard yes still bitter about this \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  pissed at time warner for causin me to have slow internet problems \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  lyx is cool \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  watching night at the museum lmao \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER URL yay happy place place place love google \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  moving to east palo alto \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  by the way totally inspired by this freaky nike commercial URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  obama administration must stop bonuses to aig ponzi schemers URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  judd apatow creates fake sitcom on nbc com to market his new movie viral marketing at its best URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just bought my tickets for the 2010 fifa world cup in south africa its going to be great summer URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER what is at amp fucking up \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  jquery is my new best friend \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER the g2 is amazing btw huge improvement over the g1 \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  glad didnt do bay to breakers today it 100 freaking degrees in san francisco wtf \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  ahh got rid of stupid time warner today amp now taking nap while the roomies cook for me pretty good end for monday EMOJIsmile \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rt USER URL great article by malcolm gladwell \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just watched night at the museum so stinkin cute \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER when lebron is done in the nba he will probably be greater than kobe like said kobe is good but there alot of good players \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  impatiently awaiting the arrival of the time warner guy it way too pretty to be inside all afternoon \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  curses the twitter api limit \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER hey no offense but malcolm gladwell is pretenious annoying cunt and he brings you down cant read his shit \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  obviously not siding with cheney here URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER USER just got us 50d for the office \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  malcolm gladwell might be my new man crush \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  awesome viral marketing for funny people URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  URL good video from google on using search options \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  off to the nike factory \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  trouble in iran see hmm iran iran so far away flockofseagullsweregeopoliticallycorrect \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  saw night at the museum out of sheer desperation who is funding these movies \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  googleio o3d bringing 3d graphics to the browser very nice tbh funfun \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER finally got around to using jquery to make my bio collapse yay for slide animations \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  after using latex lot any other typeset mathematics just looks hideous \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER fair enough but have the kindle2 and think it perfect EMOJIsmile \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  cant sleep my tooth is aching \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rt USER goodby silverstein new site URL enjoy it nice find \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  srsly hate the stupid twitter api timeout thing soo annoying EMOJIsad \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  so night at the museum was awesome much better than part next weekend we ll see up \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  atebits just finished watching your stanford iphone class session really appreciate it you rock \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  shout outs to all east palo alto for being in the buildin karizmakaze 50cal gta also thanks to profits of doom universal hempz cracka \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  viral marketing fail this acia pills brand oughta get shut down for hacking into people messenger get msgs in day arrgh \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  omg so bored amp my tattoos are so itchy help aha \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  needs someone to explain lambda calculus to him EMOJIsad \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  excited about seeing bobby flay and guy fieri tomorrow at the great american food amp music fest \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  work til 6pm lets go lakers \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  reading bill clinton fail obama win URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  safeway place is nightmare right now bumming \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER URL awesome seeing the show friday at the shoreline amphitheatre never seen nin before can wait \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER yay glad you got the phone still damn you at amp \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  dick cheney dishonest speech about torture terror and obama fred kaplan slate URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER hi just saw your stanford talk and really liked your advice just saying hi from singapore yes the videos do get around \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  night at the museum wolverine and junk food perfect monday \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  watching lebron highlights damn that niggas good \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  reading michael palin book the python years great book also recommend warren buffet amp nelson mandela bio \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  fuck no internet damn time warner \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER have you tried nike addictive \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  insects have infected my spinach plant EMOJIsad \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER lebron is beast but still cheering the til the end \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  ugh the amount of times these stupid insects have bitten me grr \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  URL awesome come back from USER via USER \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  highly recommend URL by malcolm gladwell \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  time warner cable phone reps dumber than nails ugh cable was working 10 mins ago now its not wtf \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  have to go to booz allen hamilton for 2hr meeting EMOJIsad but then get to go home EMOJIsmile \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  safeway is very rock roll tonight \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the ever amazing psyop and goodby silverstein amp partners for hp URL have to go play with after effects now \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  back from seeing star trek and night at the museum star trek was amazing but night at the museum was eh \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  ok so lots of buzz from io2009 but how lucky are they free g2 URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  seriously underestimated malcolm gladwell want to meet this dude \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  lakers tonight let go \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  ahh back in real text editing environment lt latex \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rt USER rt great USER someone sitting in the shade today because someone planted tree long time ago warren buffet \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  using linux and loving it so much nicer than windows looking forward to using the wysiwyg latex editor \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  going to the dentist later \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  stanford university facebook profile is one of the most popular official university pages URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  my exam went good USER your prayers worked \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  safari is fast EMOJIsmile even on my shitty at amp tethering \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  learning about lambda calculus EMOJIsmile \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER we love you too and don want you to die latex the devil \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  night at the museum pretty furkin good \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER hey bud EMOJIsmile np do so love my 50d although love 5d mkii more \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  top ten most watched on viral video chart love the nike mostvaluablepuppets campaign from wieden amp kennedy URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  shit hitting the fan in iran craziness indeed iranelection \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER own gm car and it is junk as far as quality compared to honda \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER lol ah my skin is itchy EMOJIsad damn lawnmowing \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the dentist lied won feel any discomort prob won even need pain pills man twippin this shit hurt how many pills can take \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER no it is too big quite happy with the kindle2 \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  kobe is the best in the world not lebron \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  booz allen hamilton has bad ass homegrown social collaboration platform way cool ttiv \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  man kinda dislike apple right now case in point the iphone 3gs wish there was video recorder app please URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  how can you not love obama he makes jokes about himself \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER time warner epic fail \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  my dad was in ny for day we ate at mesa grill last night and met bobby flay so much fun except completely lost my voice today \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER lebron is monsta and he is only 24 smh the world ain ready \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  absolutely hilarious from USER URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just created my first latex file from scratch that didn work out very well see USER it great time waster \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  history exam studying ugh \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  omg time warner ed up my internet install instead of today its now next saturday another week internet amp ehfa v9fhg fml \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  reading the tweets coming out of iran the whole thing is terrifying and incredibly sad \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  hate safeway select green tea icecream bought two cartons what waste of money gt lt \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rt USER was just told that nike layoffs started today EMOJIsad \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just applied at safeway yee \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the great indian tamasha truly will unfold from may 16 the result day for indian general election \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  malcolm gladwell is genius at tricking people into not realizing he fucking idiot \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  shaunwoo hate on aig \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  prettiest insects ever pink katydids URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER yeahh wouldn really have lived in east palo alto if could have avoided it guess it only for the summer \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  dearest USER you rich bastards the visa card you sent me doesn work why screw little guy like me \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  time warner cable slogan where calling it day at 2pm happens \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  warren buffet became for time the richest man in the united states not by working but investing in big idea which lead to the fortune \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  kobe is good bt lebron has my vote \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  with my best girl for few more hours in san francisco mmfamily is wonderful \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  ck time warner cable you cking suck balls have 700 hd tv amp my damn hd channels hardly ever come in bullshit \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  hate the effing dentist \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  wftb joining bit late my connection was down boo time warner \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  arhh it weka bug and spent almost two hours to find that out crappy me \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  time warner hd line up is crap \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  mluc09 customer innovation award winner booz allen hamilton URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the nike training club beta iphone app looks very interesting \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just picked up my new canon 50d it beautiful prepare for some seriously awesome photography \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  downloading apps for my iphone so much fun EMOJIsmile there literally is an app for just about anything \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  time warner cable is down again 3rd time since memorial day bummer \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  this dentist office is cold \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  reading my kindle2 love it lee childs is good read \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER will take you on date to see night at the museum whenever you want it looks soo good \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER world cup 2010 access damn that good look \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the times of india the wonder that is india election URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  lam so in love with bobby flay he is my favorite rt USER USER you need place in phoenix we have great peppers here \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  lebron james got in car accident guess just heard it on evening news wow cant believe it will he be ok URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  ve been sending mails like crazy today to my contacts does anyone have contact at goodby silverstein love to speak to them \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the more hear about this gm thing the more angry get billions wasted more bullshit all for something like 40k employees and all the \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  hours hours of inkscape crashing normally solid as rock hours of latex complaining at the slightest thing can take any more \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  guess ll be retiring my g1 and start using my developer g2 woot googleio \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  lebron and zydrunas are such an awesome duo \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER USER great stanford course thanks for making it available to the public really helpful and informative for starting off \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER really liked USER learning jquery book URL is worth look too \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  saw the new night at the museum and loved it next is to go see up in 3d \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  dear nike stop with the flywire that shit is waste of science and ugly love USER \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  hate revision it so boring am totally unprepared for my exam tomorrow EMOJIsad things are not looking good \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  listening to obama friggin north korea \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  got my wave sandbox invite extra excited too bad have class now but ll play with it soon enough io2009 wave \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  my dentist was wrong wrong \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER google is always good place to look should ve mentioned worked on the mustang my dad USER \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just got back from the movies went to see the new night at the museum with rachel it was good \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  argghh why won my jquery appear in safari bad safari \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER you ll love your kindle2 ve had mine for few months and never looked back the new big one is huge no need for remorse EMOJIsmile \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just had mcdonalds for dinner it was good big mac meal EMOJIwink \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  damn you north korea URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  pissed about at amp mid contract upgrade price for the iphone it 200 more not going to pay 499 for something thought was 299 \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER you will not regret going to see star trek it was awesome \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER love you danny gokey EMOJIsmile \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rt USER love the nerdy stanford human biology videos makes me miss school URL \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER really dont want at amp phone service they suck when it comes to having signal \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  time warner really picks the worst time to not work all want to do is get to mtv com so can watch the hills wtff \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  went to see the star trek movie last night very satisfying \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  mean down with notre dame if have to it good school be closer to dan enjoy it \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  noo my dvr just died and was only half way through the ea presser hate you time warner \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  tell me again why we are giving more to gm we should use that for all the programs that support the unemployed \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  time warner is the devil worst possible time for the internet to go out \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  cheney and bush are the real culprits URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER did comcast fail again \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the day never have to deal with comcast again will rank as one of the best days of my life \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  ready to drop the pretenses am forever in love with jquery and want to marry it sorry ladies this nerd is jquery spokenfor js \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER USER USER USER at amp dropped the ball and isn supporting crap with the new iphone fail att sucks \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  only one exam left and am so happy for it \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  loves twitter \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  looks like summize has gone down too many tweets from wwdc perhaps \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  know how sad is that rt USER 1st day of hurricane season that less scarey than govt taking over gm \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  life bitch and so is dick cheney p2 bipart tlot tcot hhrs gop dnc URL \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the republican party is bunch of anti abortion zealots who couldn draw flies to dump neal boortz just now on the radio \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  now can see why dave winer screams about lack of twitter api its limitations and access throttles \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rt USER new time warner slogan time warner where we make you long for the days before cable \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  is scrapbooking with nic \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  gm files bankruptcy not good sign \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  zomg have g2 \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rt USER american idol fashion adam lambert tones down danny gokey cute URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  like this guy barack obama shows his funny side gt gt URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  love dwight howard vitamin water commercial now wish he was with nike and not adidas lol \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  so tired didn sleep well at all last night \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  blink by malcolm gladwell amazing book and the tipping point \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  really loving the new search site wolfram alpha makes google seem so quaint URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  blah blah blah same old same old no plans today going back to sleep guess \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  stanford charity fashion show top draw URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER never did thank you for including me in your top 100 twitter authors you rock amp new wave URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER thanks so much from one of your very happy kindle2 winners was so surprised fabulous thank you best kathleen \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  listening to by danny gokey lt lt lt aww he so amazing lt him so much EMOJIsmile \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  annoying new trend on the internets people picking apart michael lewis and malcolm gladwell nobody wants to read that \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  highly recommend malcolm gladwell the tipping point my next audiobook will probably be one of his as well \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER lebron is beast nobody in the nba comes even close \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just got back from church and totally hate insects \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  son has me looking at cars online hate car shopping would rather go to the dentist anyone with good car at good price to sell \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER playing with twitter api sounds fun may need to take class or find new friend who like to generate results with api code \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  is twitter connections api broken some tweets didn make it to twitter \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER has been bit crazy with steep learning curve but lyx is really good for long docs for anything shorter it would be insane \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  wish could catch every mosquito in the world burn em slowly they been bitin the shit outta me 2day mosquitos are the assholes of insects \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  in montreal for long weekend of amp much needed \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the safeway bathroom still smells like ass \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  luke and got stopped walking out of safeway and asked to empty our pockets and lift our shirts how jacked up is that \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER lebron is cool like his personality he has good character \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  am loving new malcolm gladwell book outliers \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  confirmed it time warner fault not facebook that fb is taking about minutes to load so tempted to switch to verizon \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  at safeway on elkhorn they move like they re dead \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  today is good day to dislike at amp vote out of office indeed USER \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  bonjour san francisco my back hurts from last night \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  time warner road runner customer support here absolutely blows hate not having other high speed net options ready to go nuclear \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  barack obama shows his funny side gt gt URL great speech \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER love google translator too good day mate \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  would rather pay reasonable yearly taxes for free fast internet than get gouged by time warner for slow connection \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just got home from chick fil with the boys damn my internets down stupid time warner \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  html demos lots of great stuff to come yes excited EMOJIsmile URL io2009 googleio \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER such an awesome idea the continual learning program with kindle2 URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  love lebron URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  is studing math EMOJIwink tomorrow exam and dentist EMOJIsmile \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER pelosi should stay in china and never come back \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  am happy for philip being at googleio today \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  recovering from surgery wishing USER was here EMOJIsad \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  yahoo answers can be butt sometimes \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  ahh yes lol ima tell my hubby to go get me sum mcdonalds \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  watchin espn jus seen this new nike commerical with puppet lebron sh was hilarious lmao \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  found nothing at nike factory off to banana republic outlet URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  house correspondents dinner was last night whoopi barbara amp sherri went obama got standing ovation \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER firmly believe that obama pelosi have zero desire to be civil it charade and slogan but they want to destroy conservatism \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  sad day bankrupt gm \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  hate the dentist who invented them anyways \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  very interesting ad from adobe by goodby silverstein amp partners youtube adobe cs4 le sens propre URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  is going to sleep then on bike ride \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  math review im going to fail the exam \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  cox or time warner cox is cheaper and gets on dslreports tw is more expensive and gets \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  fighting with latex again \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER lebron is murdering shit \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  fuzzball is more fun than at amp URL \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER because the twitter api is slow and most client aren good \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  watching night at the museum giggling \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  lebron best athlete of our generation if not all time basketball related don want to get into inter sport debates about \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  lakers played great cannot wait for thursday night lakers vs \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  although today keynote rocked for every great announcement at amp shit on us just little bit more \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  could go for lot of mcdonalds mean lot \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  on that note hate word hate pages hate latex there said it hate latex all you texn3rds can come kill me now \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER no not itchy for now maybe later lol \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  why the hell is pelosi in freakin china and on whose dime \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  are you burning more cash than chrysler and gm stop the financial tsunami where bailout means taking handout \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  the kindle2 seems the best ereader but will it work in the uk and where can get one \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER have kindle2 amp sony prs 500 like it physical device feels good font is nice pg turns are snappy enuf ui little klunky \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  just got free g2 android at google \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  yankees won mets lost its good day \n",
            "actual label=   positive\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rt USER the upside to time warner unhelpful phone operators superslow on site service crap that not an upside \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER if you re being harassed by calls about your car warranty changing your number won fix that they call every number bags \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  USER oh snap you work at at amp don you \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  us planning to resume the military tribunals at guantanamo bay only this time those on trial will be aig execs and chrysler debt holders \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  rantsandraves the worst thing about gm concord pleasant hill martinez is the fucking uaw URL \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  obama more popular than among arabs survey president barack obama popularity in leading arab countries URL \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  night at the museum tonite instead of up EMOJIsad oh well that yr old better enjoy it lol \n",
            "actual label=   negative\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  hate comcast right now everything is down cable internet amp phone ughh what am to do \n",
            "actual label=   negative\n",
            "predicted label=  negative\n",
            "********************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected sentence:  my kindle2 came and love it EMOJIsmile \n",
            "actual label=   positive\n",
            "predicted label=  positive\n",
            "********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_confusion_matrix(y_true, y_pred , amount_classes):\n",
        "    confusion_matrix = np.zeros((amount_classes, amount_classes))\n",
        "    for idx in range(len(y_true)):\n",
        "        target = y_true[idx]\n",
        "        output = y_pred[idx]\n",
        "        confusion_matrix[target][output] += 1\n",
        "    return confusion_matrix"
      ],
      "metadata": {
        "id": "q7jNTZgMbUse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_confusion_matrix( predictions , testData['polarity'].values , 2)"
      ],
      "metadata": {
        "id": "PQtAvNkYbX5W",
        "outputId": "b46f41ce-2276-4ab4-fa5e-24c80a49dc56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[117.,  53.],\n",
              "       [ 60., 128.]])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.BuPu):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = plt.axes()\n",
        "    \n",
        "    ax.xaxis.set_ticks_position('top')\n",
        "    ax.xaxis.set_label_position('top')\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "adUcZchWqffv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(create_confusion_matrix( predictions , testData['polarity'].values , 2), ['negative' , 'positive'], normalize=False, title='Confusion matrix', cmap=plt.cm.Greens)\n"
      ],
      "metadata": {
        "id": "MrdnxtI_3Chs",
        "outputId": "5dd58d69-d454-459d-c95f-d8df79234289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8S4JLKLq3fRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c244499731494fef8754ceae12c1793e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a7cd909c1e2149e5a0e23957655b6522",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_97840aad44bf45db8af0d90c630b1f39",
              "IPY_MODEL_dca5ef8d2bed478884cf32d1a68ef8e7",
              "IPY_MODEL_0e80889898f4485a996fe80c44fc022c"
            ]
          }
        },
        "a7cd909c1e2149e5a0e23957655b6522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97840aad44bf45db8af0d90c630b1f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3fb51e5626054ca0bb0fd5fdbc889b05",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13be6818b28c4750bf63bb2fd404019a"
          }
        },
        "dca5ef8d2bed478884cf32d1a68ef8e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d7985c41f0454aae85405358447d9cef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ba23c1f353540499b92b8621a105731"
          }
        },
        "0e80889898f4485a996fe80c44fc022c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c6433b44194144d0bb947c0b27289fb3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [58:45&lt;00:00, 882.62s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea4cb909591e4219a4f7f02fc7f48740"
          }
        },
        "3fb51e5626054ca0bb0fd5fdbc889b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13be6818b28c4750bf63bb2fd404019a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7985c41f0454aae85405358447d9cef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ba23c1f353540499b92b8621a105731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6433b44194144d0bb947c0b27289fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea4cb909591e4219a4f7f02fc7f48740": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8283b7b96fef4cba9262ce0f5576bf4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_50efdedc977a42808251f517c51f110d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a2cfbdd82fd4093a8cee3e7f1e8fbf4",
              "IPY_MODEL_138e2aad057841dcb3323e1919a5062f",
              "IPY_MODEL_5238a60e80c241b2b1b011ddff8e71d9"
            ]
          }
        },
        "50efdedc977a42808251f517c51f110d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a2cfbdd82fd4093a8cee3e7f1e8fbf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a85bb75932bb4eb7b2ae51a8c7197738",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 1: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d853dbb8191743ac903f92a4b1c8beec"
          }
        },
        "138e2aad057841dcb3323e1919a5062f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_37b0554289d245e791726554f0300bea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1250,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_689f0bf5654d40678457e0df725e19e4"
          }
        },
        "5238a60e80c241b2b1b011ddff8e71d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2504636c4a78435c98c92461788f2902",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1250/1250 [13:19&lt;00:00,  1.54it/s, training_loss=0.105]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fa6b5abaa22842c2b89268ec5930ce43"
          }
        },
        "a85bb75932bb4eb7b2ae51a8c7197738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d853dbb8191743ac903f92a4b1c8beec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37b0554289d245e791726554f0300bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "689f0bf5654d40678457e0df725e19e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2504636c4a78435c98c92461788f2902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fa6b5abaa22842c2b89268ec5930ce43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "094441f51c264c9a8cff68d79f32f56f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7f9f5f78f6904377af4926e50c2444c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_502612f319ab4e988547188e4e2f96e3",
              "IPY_MODEL_6324f50557b74eff9a2d2c163f458a20",
              "IPY_MODEL_19846a40951e48d7b5f66a40f77afad6"
            ]
          }
        },
        "7f9f5f78f6904377af4926e50c2444c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "502612f319ab4e988547188e4e2f96e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_43122270965d44079223700bc3e9f583",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 2: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47c0b6cf212748908e3db291f558a217"
          }
        },
        "6324f50557b74eff9a2d2c163f458a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_23023de4995c477bbd6ef9c5b667855d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1250,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2afc5e72923940bda9e19307d51a5fd3"
          }
        },
        "19846a40951e48d7b5f66a40f77afad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dabafaf292414b9f8b11ac3923ba56ad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1250/1250 [13:31&lt;00:00,  1.54it/s, training_loss=0.080]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21e1868c2a414326be41adbd4b2c0435"
          }
        },
        "43122270965d44079223700bc3e9f583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47c0b6cf212748908e3db291f558a217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23023de4995c477bbd6ef9c5b667855d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2afc5e72923940bda9e19307d51a5fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dabafaf292414b9f8b11ac3923ba56ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21e1868c2a414326be41adbd4b2c0435": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21fc96744440451998899eff00e9c2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c177badb09f14062822a882357f63303",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7411c21bacd74564b323fa7c14bc18bf",
              "IPY_MODEL_8b139b9bf7944caaaf25f36382640c9d",
              "IPY_MODEL_8c7a76d10e774ce5aa2f9329114f371d"
            ]
          }
        },
        "c177badb09f14062822a882357f63303": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7411c21bacd74564b323fa7c14bc18bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_385ee47cd082466ba49fe23e48b57c93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 3: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c7e8f6985c0488599549eec012e5187"
          }
        },
        "8b139b9bf7944caaaf25f36382640c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1cb58f3f35564d50b5b4031417f29c85",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1250,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e33fe405e76451e8caa5bb898f30616"
          }
        },
        "8c7a76d10e774ce5aa2f9329114f371d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f4d7b70d27474a2a8c561714ab7f9d3b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1250/1250 [13:31&lt;00:00,  1.54it/s, training_loss=0.086]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_44e963627f8844d2b564dbf2fd157b8e"
          }
        },
        "385ee47cd082466ba49fe23e48b57c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c7e8f6985c0488599549eec012e5187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cb58f3f35564d50b5b4031417f29c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e33fe405e76451e8caa5bb898f30616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f4d7b70d27474a2a8c561714ab7f9d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "44e963627f8844d2b564dbf2fd157b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86bb74860ec04cd085d1b9db531e37bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7db8db6d4e534c4f92990525c187568c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0c7733533cda48f68a3abbd463bd9b99",
              "IPY_MODEL_e8368a1c90034c1c9e90bc53bc5925a8",
              "IPY_MODEL_2bb60c6680b642c58ca338e76dee8c5c"
            ]
          }
        },
        "7db8db6d4e534c4f92990525c187568c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c7733533cda48f68a3abbd463bd9b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c89590ee875f4e75bf3e53deb63a544b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 4: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60afa2ec1a344dca92ec7d949cb599f4"
          }
        },
        "e8368a1c90034c1c9e90bc53bc5925a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_013f0a8214f04644b3d000eeea92dd04",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1250,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1250,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4cc4bf368c66426eb45817f0d709ae13"
          }
        },
        "2bb60c6680b642c58ca338e76dee8c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7a563b6c2d204364b4a551ec4980c186",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1250/1250 [13:31&lt;00:00,  1.54it/s, training_loss=0.031]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3992c196225642b78833c52f10fe0632"
          }
        },
        "c89590ee875f4e75bf3e53deb63a544b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60afa2ec1a344dca92ec7d949cb599f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "013f0a8214f04644b3d000eeea92dd04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4cc4bf368c66426eb45817f0d709ae13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a563b6c2d204364b4a551ec4980c186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3992c196225642b78833c52f10fe0632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}